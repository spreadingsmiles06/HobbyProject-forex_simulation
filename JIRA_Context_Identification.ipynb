{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd0667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6361f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning function\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s-]', '', text)  # Keep hyphens\n",
    "    text = ' '.join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def extract_key_phrases(text, common_terms):\n",
    "    \"\"\"Extract meaningful 2-4 word phrases\"\"\"\n",
    "    words = [w for w in clean_text(text).split() \n",
    "             if w not in common_terms and len(w) > 2]\n",
    "    \n",
    "    # Create 2-4 word phrases\n",
    "    phrases = []\n",
    "    for i in range(len(words)-1):\n",
    "        phrases.append(' '.join(words[i:i+2]))\n",
    "        if i < len(words)-2:\n",
    "            phrases.append(' '.join(words[i:i+3]))\n",
    "        if i < len(words)-3:\n",
    "            phrases.append(' '.join(words[i:i+4]))\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "def identify_module(text, common_terms, module_keywords):\n",
    "    \"\"\"Identify the most relevant module\"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # First check for exact module matches\n",
    "    for module, keywords in module_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return module\n",
    "    \n",
    "    # Then check for partial matches\n",
    "    for module, keywords in module_keywords.items():\n",
    "        if any(keyword.split()[0] in text for keyword in keywords):\n",
    "            return module\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "def get_concise_context(row, common_terms, module_keywords):\n",
    "    \"\"\"Get 2-4 word context and module\"\"\"\n",
    "    # Priority fields for context\n",
    "    summary = str(row['Summary']) if pd.notna(row['Summary']) else \"\"\n",
    "    parent_summary = str(row['Parent summary']) if 'Parent summary' in row and pd.notna(row['Parent summary']) else \"\"\n",
    "    \n",
    "    # Extract phrases from both fields\n",
    "    summary_phrases = extract_key_phrases(summary, common_terms)\n",
    "    parent_phrases = extract_key_phrases(parent_summary, common_terms)\n",
    "    \n",
    "    all_phrases = summary_phrases + parent_phrases\n",
    "    \n",
    "    if not all_phrases:\n",
    "        return \"Not clear\", \"Other\"\n",
    "    \n",
    "    # Get most common phrase\n",
    "    context = Counter(all_phrases).most_common(1)[0][0]\n",
    "    \n",
    "    # Identify module\n",
    "    combined_text = f\"{context} {parent_summary}\".strip()\n",
    "    module = identify_module(combined_text, common_terms, module_keywords)\n",
    "    \n",
    "    return context, module\n",
    "\n",
    "def group_by_module(df):\n",
    "    \"\"\"Group issues by identified module\"\"\"\n",
    "    # Create numeric groups for each module\n",
    "    modules = df['Module'].unique()\n",
    "    module_to_group = {module: i+1 for i, module in enumerate(modules)}\n",
    "    df['Module_Group'] = df['Module'].map(module_to_group)\n",
    "    return df\n",
    "\n",
    "def process_jira_file(input_path, output_path):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    # Common terms that don't represent meaningful context\n",
    "    common_terms = {\n",
    "        'the', 'and', 'for', 'with', 'this', 'that', 'issue', 'bug', \n",
    "        'fix', 'error', 'problem', 'request', 'ticket', 'work', 'task',\n",
    "        'story', 'be', 'fe', 'web', 'sms', 'email', 'high', 'open', 'uat',\n",
    "        'configuration', 'handling', 'check', 'flow', 'terms', 'conditions'\n",
    "    }\n",
    "    \n",
    "    # Define known modules and their keywords\n",
    "    module_keywords = {\n",
    "        \"Onboarding\": [\"onboard\", \"kyc\", \"registration\", \"signup\"],\n",
    "        \"Trade\": [\"trade\", \"import\", \"export\", \"lc\", \"letter of credit\"],\n",
    "        \"Trade & Remittance\": [\"remittance\", \"inward\", \"outward\", \"forex\"],\n",
    "        \"Payments\": [\"payment\", \"settle\", \"invoice\", \"collection\"],\n",
    "        \"Martech\": [\"martech\", \"moengage\", \"events\", \"integration\"],\n",
    "        \"IRM\": [\"irm\", \"stp\", \"scrutiny\", \"purpose code\"],\n",
    "        \"Indie Business\": [\"indie\", \"business\", \"ifb\", \"sms\", \"email\"]\n",
    "    }\n",
    "    \n",
    "    # Read Excel file\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Verify required columns exist\n",
    "    if 'Summary' not in df.columns:\n",
    "        print(\"Error: 'Summary' column is required\")\n",
    "        return\n",
    "    \n",
    "    # Add Context and Module columns\n",
    "    df['Context'], df['Module'] = zip(*df.apply(\n",
    "        lambda row: get_concise_context(row, common_terms, module_keywords), axis=1))\n",
    "    \n",
    "    # Group by module\n",
    "    df = group_by_module(df)\n",
    "    \n",
    "    # Sort by module group and context\n",
    "    df_sorted = df.sort_values(['Module_Group', 'Context'])\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        df_sorted.to_excel(output_path, index=False)\n",
    "        print(f\"Success! Processed data saved to {output_path}\")\n",
    "        print(f\"Total issues processed: {len(df)}\")\n",
    "        print(f\"Modules identified: {', '.join(df['Module'].unique())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d93d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed data saved to c:\\Users\\HomeSmiles\\Downloads\\grouped_jira_issues.xlsx\n",
      "Total issues processed: 259\n",
      "Modules identified: Payments, Other, Martech, Indie Business, IRM, Onboarding, Trade, Trade & Remittance\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"c:\\Users\\HomeSmiles\\Downloads\\IFB-MSME-Sprint.xlsx\"  # Your input file\n",
    "    output_file = r\"c:\\Users\\HomeSmiles\\Downloads\\grouped_jira_issues.xlsx\"  # Output file\n",
    "    process_jira_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c542b",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471f6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78958ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "# Load English language model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "except:\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_md'])\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "class JIRAAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.common_terms = {\n",
    "            'the', 'and', 'for', 'with', 'this', 'that', 'issue', 'bug',\n",
    "            'fix', 'error', 'problem', 'request', 'ticket', 'work', 'task',\n",
    "            'story', 'be', 'fe', 'web', 'high', 'open', 'uat', 'system'\n",
    "        }\n",
    "        \n",
    "        # Domain-agnostic module template (customize as needed)\n",
    "        self.module_keywords = {\n",
    "            \"Core Module 1\": [\"primary function 1\", \"system component 1\"],\n",
    "            \"Core Module 2\": [\"primary function 2\", \"system component 2\"],\n",
    "            \"Integration\": [\"api integration\", \"system connection\"],\n",
    "            \"Security\": [\"access control\", \"authentication\"],\n",
    "            \"UI/UX\": [\"user interface\", \"screen design\"]\n",
    "        }\n",
    "\n",
    "    def set_domain(self, domain_name, module_specs):\n",
    "        \"\"\"Configure for specific domain (e.g., banking, e-commerce)\"\"\"\n",
    "        self.domain = domain_name\n",
    "        self.module_keywords = module_specs\n",
    "        \n",
    "        # Add domain-specific common terms\n",
    "        if domain_name.lower() == \"banking\":\n",
    "            self.common_terms.update({\n",
    "                'payment', 'transaction', 'account', 'customer', 'bank'\n",
    "            })\n",
    "        elif domain_name.lower() == \"ecommerce\":\n",
    "            self.common_terms.update({\n",
    "                'product', 'cart', 'checkout', 'inventory', 'order'\n",
    "            })\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Domain-agnostic text cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s-]', '', text)\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def extract_phrases(self, text):\n",
    "        \"\"\"Extract 2-4 word phrases with NLP\"\"\"\n",
    "        doc = nlp(text)\n",
    "        phrases = set()\n",
    "        \n",
    "        # Extract noun chunks (most descriptive)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            words = [w.text for w in chunk if not w.is_stop and len(w.text) > 2]\n",
    "            if 2 <= len(words) <= 4:\n",
    "                phrases.add(' '.join(words))\n",
    "        \n",
    "        # Fallback to important entities\n",
    "        if not phrases:\n",
    "            for ent in doc.ents:\n",
    "                if 2 <= len(ent.text.split()) <= 4:\n",
    "                    phrases.add(ent.text.lower())\n",
    "        \n",
    "        # Final fallback to key words\n",
    "        if not phrases:\n",
    "            words = [token.text for token in doc \n",
    "                    if not token.is_stop and len(token.text) > 2]\n",
    "            for i in range(len(words)-1):\n",
    "                if i+3 < len(words):\n",
    "                    phrases.add(' '.join(words[i:i+4]))\n",
    "                elif i+2 < len(words):\n",
    "                    phrases.add(' '.join(words[i:i+3]))\n",
    "                else:\n",
    "                    phrases.add(' '.join(words[i:i+2]))\n",
    "        \n",
    "        return [p for p in phrases if 2 <= len(p.split()) <= 4]\n",
    "\n",
    "    def identify_module(self, text):\n",
    "        \"\"\"Flexible module identification\"\"\"\n",
    "        doc = nlp(text)\n",
    "        best_match = (\"Other\", 0)\n",
    "        \n",
    "        for module, keywords in self.module_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                similarity = doc.similarity(nlp(keyword))\n",
    "                if similarity > best_match[1]:\n",
    "                    best_match = (module, similarity)\n",
    "        \n",
    "        return best_match[0] if best_match[1] > 0.7 else \"Other\"\n",
    "\n",
    "    def analyze_row(self, row):\n",
    "        \"\"\"Process single JIRA row\"\"\"\n",
    "        summary = self.clean_text(row['Summary'])\n",
    "        parent = self.clean_text(row.get('Parent Summary', ''))\n",
    "        \n",
    "        # Extract context (guaranteed 2-4 words)\n",
    "        phrases = self.extract_phrases(f\"{summary} {parent}\")\n",
    "        context = max(phrases, key=len) if phrases else \"Not clear\"\n",
    "        \n",
    "        # Identify module\n",
    "        module = self.identify_module(f\"{context} {parent}\")\n",
    "        \n",
    "        return context[:100], module  # Ensure reasonable length\n",
    "\n",
    "    def process_file(self, input_path, output_path):\n",
    "        \"\"\"End-to-end processing\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(input_path)\n",
    "            \n",
    "            # Add analysis columns\n",
    "            df['Context'], df['Module'] = zip(*df.apply(\n",
    "                lambda row: self.analyze_row(row), axis=1))\n",
    "            \n",
    "            # Group similar items\n",
    "            self._group_similar_issues(df)\n",
    "            \n",
    "            # Save results\n",
    "            df.to_excel(output_path, index=False)\n",
    "            print(f\"Analysis complete. Results saved to {output_path}\")\n",
    "            self._print_summary(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "    def _group_similar_issues(self, df):\n",
    "        \"\"\"Group issues by context similarity\"\"\"\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf = vectorizer.fit_transform(df['Context'])\n",
    "        \n",
    "        df['Similarity_Group'] = 0\n",
    "        group_id = 1\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if df.at[i, 'Similarity_Group'] == 0:\n",
    "                df.at[i, 'Similarity_Group'] = group_id\n",
    "                for j in range(i+1, len(df)):\n",
    "                    if cosine_similarity(tfidf[i], tfidf[j])[0][0] > 0.65:\n",
    "                        df.at[j, 'Similarity_Group'] = group_id\n",
    "                group_id += 1\n",
    "\n",
    "    def _print_summary(self, df):\n",
    "        \"\"\"Print analysis summary\"\"\"\n",
    "        print(\"\\n=== Analysis Summary ===\")\n",
    "        print(f\"Total Issues: {len(df)}\")\n",
    "        print(\"\\nModule Distribution:\")\n",
    "        print(df['Module'].value_counts())\n",
    "        \n",
    "        if \"Other\" in df['Module'].values:\n",
    "            print(\"\\nSample 'Other' Contexts:\")\n",
    "            print(df[df['Module'] == \"Other\"]['Context'].head(3).to_string(index=False))\n",
    "\n",
    "# Example configuration for banking domain\n",
    "banking_modules = {\n",
    "    \"Positive Pay\": [\"check fraud prevention\", \"payee verification\"],\n",
    "    \"Loan Processing\": [\"loan approval\", \"credit assessment\"],\n",
    "    \"Digital Banking\": [\"mobile app\", \"online banking\"],\n",
    "    \"Compliance\": [\"aml screening\", \"kyc verification\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7dde34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomeSmiles\\AppData\\Local\\Temp\\ipykernel_22812\\2082870188.py:94: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = doc.similarity(nlp(keyword))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to c:\\Users\\HomeSmiles\\Downloads\\jira_nlp_enhanced.xlsx\n",
      "\n",
      "=== Analysis Summary ===\n",
      "Total Issues: 259\n",
      "\n",
      "Module Distribution:\n",
      "Module\n",
      "Other              212\n",
      "Positive Pay        31\n",
      "Digital Banking     16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample 'Other' Contexts:\n",
      "       sorted order\n",
      "addmanage bene list\n",
      "   trade remittance\n"
     ]
    }
   ],
   "source": [
    "# Usage Example:\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = JIRAAnalyzer()\n",
    "    \n",
    "    # Configure for banking domain (Positive Pay is just one module)\n",
    "    analyzer.set_domain(\"banking\", banking_modules)\n",
    "    \n",
    "    # Process files\n",
    "    analyzer.process_file(\n",
    "        input_path = r\"c:\\Users\\HomeSmiles\\Downloads\\IFB-MSME-Sprint.xlsx\",\n",
    "        output_path = r\"c:\\Users\\HomeSmiles\\Downloads\\jira_nlp_enhanced.xlsx\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf40a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
